name: Scrape Greenhouse Jobs (Free)

on:
  # æ¯å¤©å‡Œæ™¨3ç‚¹UTCè‡ªåŠ¨è¿è¡Œ
  schedule:
    - cron: '0 3 * * *'
  
  # æ”¯æŒæ‰‹åŠ¨è§¦å‘
  workflow_dispatch:
    inputs:
      start_index:
        description: 'Start index (for resume)'
        required: false
        default: '0'

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Run scraper
        run: |
          python fetch_greenhouse_jobs.py \
            --companies companies.txt \
            --output data \
            --start ${{ github.event.inputs.start_index || '0' }} \
            --delay 2
      
      # æ–¹æ¡ˆ1ï¼šæäº¤å›žGitHubä»“åº“ï¼ˆå…è´¹æ— é™ç©ºé—´ï¼‰
      - name: Commit and push data
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git add data/
          git diff --staged --quiet || git commit -m "Update jobs data $(date +'%Y-%m-%d %H:%M')"
          git push
      
      # æ–¹æ¡ˆ2ï¼šä¸Šä¼ ä¸ºArtifactï¼ˆä¿ç•™90å¤©ï¼Œå…è´¹ï¼‰
      - name: Upload jobs as artifact
        uses: actions/upload-artifact@v3
        with:
          name: greenhouse-jobs-${{ github.run_number }}
          path: data/
          retention-days: 90
      
      # å¯é€‰ï¼šç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
      - name: Generate summary
        run: |
          if [ -f data/_stats.json ]; then
            echo "## Scraping Summary" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            python3 -c "
            import json
            with open('data/_stats.json') as f:
                stats = json.load(f)
            print(f\"âœ… **Success**: {stats['success']}\")
            print(f\"âŒ **Failed**: {stats['failed']}\")
            print(f\"ðŸ“Š **Total Jobs**: {stats['total_jobs']}\")
            " >> $GITHUB_STEP_SUMMARY
          fi

