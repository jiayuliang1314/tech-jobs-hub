name: Auto Scrape Jobs

on:
  # æ¯å¤©å‡Œæ™¨3ç‚¹UTCè‡ªåŠ¨è¿è¡Œï¼ˆåŒ—äº¬æ—¶é—´11ç‚¹ï¼‰
  schedule:
    - cron: '0 3 * * *'
  
  # æ‰‹åŠ¨è§¦å‘
  workflow_dispatch:
    inputs:
      batch_size:
        description: 'Number of companies per run'
        required: false
        default: '1000'

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Scrape jobs
        run: |
          echo "ğŸš€ Starting job scraping..."
          echo "ğŸ“Š Total companies: $(wc -l < companies_tokens_only.txt)"
          
          # æ¯å¤©æŠ“å–1000å®¶å…¬å¸
          # 2754å®¶ Ã· 1000 = 3å¤©æŠ“å–å®Œæˆ
          
          # è®¡ç®—ä»Šå¤©åº”è¯¥æŠ“å–å“ªä¸€æ‰¹
          DAY_OF_YEAR=$(date +%j)
          BATCH_NUM=$((DAY_OF_YEAR % 3))
          START=$((BATCH_NUM * 1000 + 1))
          END=$((START + 999))
          
          echo "ğŸ“… Today is day $DAY_OF_YEAR, running batch $BATCH_NUM"
          echo "ğŸ“‹ Scraping companies $START to $END"
          
          # æå–å¯¹åº”æ‰¹æ¬¡çš„å…¬å¸
          sed -n "${START},${END}p" companies_tokens_only.txt > batch_today.txt
          
          # æ‰§è¡ŒæŠ“å–
          python3 fetch_greenhouse_jobs.py \
            --companies batch_today.txt \
            --output data \
            --delay 1.5
          
          echo "âœ… Scraping completed"
      
      - name: Build search index
        run: |
          echo "ğŸ”¨ Building search index..."
          python3 build_search_index.py --data data --output .
          
          # å¤åˆ¶åˆ°æ ¹ç›®å½•ï¼ˆGitHub Pagesè®¿é—®ï¼‰
          cp search-index.json search-meta.json companies-list.json locations-list.json ./
          cp -r indexes ./
          
          echo "âœ… Search index built"
      
      - name: Update statistics
        run: |
          echo "ğŸ“Š Generating statistics..."
          python3 << 'PYSCRIPT'
          import json
          from pathlib import Path
          from datetime import datetime
          
          total_jobs = 0
          total_companies = 0
          
          for f in Path('data').glob('*.json'):
              if f.name.startswith('_'):
                  continue
              try:
                  with open(f) as file:
                      data = json.load(file)
                      total_jobs += data.get('job_count', 0)
                      total_companies += 1
              except:
                  pass
          
          stats = {
              'total_jobs': total_jobs,
              'total_companies': total_companies,
              'last_updated': datetime.utcnow().isoformat() + 'Z',
              'next_update': 'Daily at 3:00 UTC'
          }
          
          with open('stats.json', 'w') as f:
              json.dump(stats, f, indent=2)
          
          print(f'âœ… Total: {total_jobs} jobs from {total_companies} companies')
          PYSCRIPT
      
      - name: Commit and push updates
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          
          git add data/ search-index.json search-meta.json companies-list.json locations-list.json indexes/ stats.json
          
          # åªåœ¨æœ‰å˜åŒ–æ—¶æäº¤
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Auto update: $(date '+%Y-%m-%d %H:%M UTC') [skip ci]"
            git push
            echo "âœ… Changes pushed to repository"
          fi
      
      - name: Summary
        run: |
          echo "## ğŸ“Š Scraping Summary" >> $GITHUB_STEP_SUMMARY
          cat stats.json >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ğŸŒ Website: https://jiayuliang1314.github.io/tech-jobs-hub/" >> $GITHUB_STEP_SUMMARY

