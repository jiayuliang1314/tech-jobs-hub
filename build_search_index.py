#!/usr/bin/env python3
"""
æ„å»ºæœç´¢ç´¢å¼• - é›¶æˆæœ¬æ–¹æ¡ˆ
å°†100,000ä¸ªèŒä½è½¬æ¢ä¸ºè½»é‡çº§æœç´¢ç´¢å¼•ï¼ˆ20MBï¼‰
æ”¯æŒå‰ç«¯å¿«é€Ÿæœç´¢ï¼Œæ— éœ€åç«¯æ•°æ®åº“
"""

import json
from pathlib import Path
from datetime import datetime
import re

class SearchIndexBuilder:
    def __init__(self, data_dir='test_data', output_dir='html-demo'):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.stats = {
            'total_jobs': 0,
            'total_companies': 0,
            'index_size': 0
        }
    
    def build_index(self):
        """æ„å»ºæœç´¢ç´¢å¼•"""
        
        print("ğŸ”¨ Building search index...")
        print(f"ğŸ“ Data dir: {self.data_dir}")
        print(f"ğŸ“ Output dir: {self.output_dir}\n")
        
        # å®Œæ•´ç´¢å¼•
        full_index = []
        
        # æŒ‰å­—æ¯åˆ†ç»„ï¼ˆä¼˜åŒ–åŠ è½½ï¼‰
        alpha_indexes = {}
        
        # å…¬å¸åˆ—è¡¨ï¼ˆç”¨äºè‡ªåŠ¨å®Œæˆï¼‰
        companies = set()
        
        # åœ°ç‚¹åˆ—è¡¨ï¼ˆç”¨äºè‡ªåŠ¨å®Œæˆï¼‰
        locations = set()
        
        # éå†æ‰€æœ‰å…¬å¸æ–‡ä»¶
        for company_file in sorted(self.data_dir.glob('*.json')):
            if company_file.name.startswith('_'):
                continue
            
            try:
                with open(company_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                company = data.get('company', company_file.stem)
                jobs = data.get('jobs', [])
                
                companies.add(company)
                
                for job in jobs:
                    # åˆ›å»ºè½»é‡çº§ç´¢å¼•æ¡ç›®
                    entry = {
                        'id': str(job['id']),
                        'title': job['title'],
                        'company': company,
                        'location': job['location'],
                        'url': job['url'],
                        # æœç´¢å…³é”®è¯ï¼ˆå°å†™ï¼Œç”¨äºå¿«é€ŸåŒ¹é…ï¼‰
                        '_k': self.generate_keywords(job['title'], company, job['location'])
                    }
                    
                    full_index.append(entry)
                    
                    # æŒ‰é¦–å­—æ¯åˆ†ç»„
                    first_letter = job['title'][0].upper() if job['title'] else 'Z'
                    if not first_letter.isalpha():
                        first_letter = 'Z'
                    
                    if first_letter not in alpha_indexes:
                        alpha_indexes[first_letter] = []
                    alpha_indexes[first_letter].append(entry)
                    
                    # æ”¶é›†åœ°ç‚¹
                    city = job['location'].split(',')[0].strip()
                    locations.add(city)
                    
                    self.stats['total_jobs'] += 1
                
                self.stats['total_companies'] += 1
                
                if self.stats['total_jobs'] % 500 == 0:
                    print(f"  â³ Processed {self.stats['total_jobs']} jobs...")
                    
            except Exception as e:
                print(f"  âŒ Error processing {company_file.name}: {e}")
        
        print(f"\nâœ… Indexed {self.stats['total_jobs']} jobs from {self.stats['total_companies']} companies")
        
        # ä¿å­˜ç´¢å¼•
        self.save_indexes(full_index, alpha_indexes, companies, locations)
        
        return self.stats
    
    def generate_keywords(self, title, company, location):
        """ç”Ÿæˆæœç´¢å…³é”®è¯"""
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        text = f"{title} {company} {location}".lower()
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = ' '.join(text.split())
        
        return text
    
    def save_indexes(self, full_index, alpha_indexes, companies, locations):
        """ä¿å­˜ç´¢å¼•æ–‡ä»¶"""
        
        print("\nğŸ’¾ Saving indexes...\n")
        
        # 1. ä¸»æœç´¢ç´¢å¼•ï¼ˆå‹ç¼©JSONï¼‰
        main_index_path = self.output_dir / 'search-index.json'
        with open(main_index_path, 'w', encoding='utf-8') as f:
            json.dump({
                'total': len(full_index),
                'generated_at': datetime.now().isoformat(),
                'jobs': full_index
            }, f, ensure_ascii=False, separators=(',', ':'))
        
        size_mb = main_index_path.stat().st_size / 1024 / 1024
        print(f"  âœ… Main index: {main_index_path.name} ({size_mb:.1f}MB)")
        self.stats['index_size'] = size_mb
        
        # 2. æŒ‰å­—æ¯åˆ†ç»„çš„ç´¢å¼•ï¼ˆå¯é€‰ï¼Œä¼˜åŒ–é¦–æ¬¡åŠ è½½ï¼‰
        alpha_dir = self.output_dir / 'indexes'
        alpha_dir.mkdir(exist_ok=True)
        
        for letter, jobs in alpha_indexes.items():
            letter_path = alpha_dir / f'{letter}.json'
            with open(letter_path, 'w', encoding='utf-8') as f:
                json.dump(jobs, f, ensure_ascii=False, separators=(',', ':'))
        
        print(f"  âœ… Alpha indexes: {len(alpha_indexes)} files in indexes/")
        
        # 3. å…¬å¸åˆ—è¡¨ï¼ˆç”¨äºè‡ªåŠ¨å®Œæˆï¼‰
        companies_path = self.output_dir / 'companies-list.json'
        with open(companies_path, 'w', encoding='utf-8') as f:
            json.dump(sorted(list(companies)), f, ensure_ascii=False)
        
        print(f"  âœ… Companies list: {len(companies)} companies")
        
        # 4. åœ°ç‚¹åˆ—è¡¨ï¼ˆç”¨äºè‡ªåŠ¨å®Œæˆï¼‰
        locations_path = self.output_dir / 'locations-list.json'
        with open(locations_path, 'w', encoding='utf-8') as f:
            json.dump(sorted(list(locations)), f, ensure_ascii=False)
        
        print(f"  âœ… Locations list: {len(locations)} locations")
        
        # 5. å…ƒæ•°æ®
        meta_path = self.output_dir / 'search-meta.json'
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump({
                'total_jobs': self.stats['total_jobs'],
                'total_companies': self.stats['total_companies'],
                'total_locations': len(locations),
                'index_size_mb': round(size_mb, 2),
                'generated_at': datetime.now().isoformat(),
                'files': {
                    'main': 'search-index.json',
                    'alpha': 'indexes/{A-Z}.json',
                    'companies': 'companies-list.json',
                    'locations': 'locations-list.json'
                }
            }, f, indent=2)
        
        print(f"  âœ… Metadata: search-meta.json")
        print(f"\nğŸ‰ Search index built successfully!")

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='Build search index')
    parser.add_argument('--data', default='test_data', help='Data directory')
    parser.add_argument('--output', default='html-demo', help='Output directory')
    
    args = parser.parse_args()
    
    builder = SearchIndexBuilder(data_dir=args.data, output_dir=args.output)
    stats = builder.build_index()
    
    print(f"\nğŸ“Š Final Stats:")
    print(f"  Jobs: {stats['total_jobs']:,}")
    print(f"  Companies: {stats['total_companies']}")
    print(f"  Index size: {stats['index_size']:.2f}MB")
    print(f"\nğŸ’¡ Next: Integrate search-index.json into your website")

if __name__ == '__main__':
    main()

