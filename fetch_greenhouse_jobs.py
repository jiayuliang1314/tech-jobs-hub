#!/usr/bin/env python3
"""
Greenhouse Jobs Scraper - é›¶æˆæœ¬æ–¹æ¡ˆ
æ”¯æŒ3000å®¶å…¬å¸æ‰¹é‡æŠ“å–ï¼Œä½¿ç”¨å…è´¹GitHub Actionsè¿è¡Œ
"""

import json
import time
import urllib.request
import urllib.error
from pathlib import Path
from datetime import datetime
import sys

class GreenHouseScraper:
    def __init__(self, output_dir='data'):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.base_url = 'https://boards-api.greenhouse.io/v1/boards/{}/jobs'
        self.single_job_url = 'https://boards-api.greenhouse.io/v1/boards/{}/jobs/{}'
        self.stats = {
            'success': 0,
            'failed': 0,
            'total_jobs': 0,
            'errors': []
        }
    
    def fetch_company_jobs(self, board_token, retry=3):
        """æŠ“å–å•ä¸ªå…¬å¸çš„èŒä½"""
        url = self.base_url.format(board_token)
        
        for attempt in range(retry):
            try:
                # è®¾ç½®User-Agenté¿å…è¢«æ‹¦æˆª
                req = urllib.request.Request(
                    url,
                    headers={'User-Agent': 'Mozilla/5.0 (compatible; JobAggregator/1.0)'}
                )
                
                with urllib.request.urlopen(req, timeout=30) as response:
                    data = json.loads(response.read().decode('utf-8'))
                    jobs = data.get('jobs', [])
                    
                    if jobs:
                        self.stats['success'] += 1
                        self.stats['total_jobs'] += len(jobs)
                        print(f"âœ… {board_token}: {len(jobs)} jobs")
                        return jobs
                    else:
                        print(f"âš ï¸  {board_token}: 0 jobs")
                        return []
                        
            except urllib.error.HTTPError as e:
                if e.code == 404:
                    print(f"âŒ {board_token}: Not found (404)")
                    self.stats['failed'] += 1
                    self.stats['errors'].append({'company': board_token, 'error': '404'})
                    return None
                elif attempt < retry - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    print(f"âŒ {board_token}: HTTP {e.code}")
                    self.stats['failed'] += 1
                    return None
                    
            except Exception as e:
                if attempt < retry - 1:
                    time.sleep(2)
                    continue
                else:
                    print(f"âŒ {board_token}: {str(e)[:50]}")
                    self.stats['failed'] += 1
                    self.stats['errors'].append({'company': board_token, 'error': str(e)[:100]})
                    return None
        
        return None
    
    def fetch_job_details(self, board_token, job_id, retry=2):
        """è·å–å•ä¸ªèŒä½çš„å®Œæ•´ä¿¡æ¯ï¼ˆåŒ…æ‹¬æè¿°ï¼‰"""
        url = self.single_job_url.format(board_token, job_id)
        
        for attempt in range(retry):
            try:
                req = urllib.request.Request(
                    url,
                    headers={'User-Agent': 'Mozilla/5.0 (compatible; JobAggregator/1.0)'}
                )
                
                with urllib.request.urlopen(req, timeout=20) as response:
                    data = json.loads(response.read().decode('utf-8'))
                    return data
                    
            except Exception as e:
                if attempt < retry - 1:
                    time.sleep(1)
                    continue
                else:
                    return None
        
        return None
    
    def save_jobs(self, board_token, jobs, fetch_details=False):
        """ä¿å­˜èŒä½åˆ°JSONæ–‡ä»¶
        
        Args:
            board_token: å…¬å¸æ ‡è¯†
            jobs: èŒä½åˆ—è¡¨
            fetch_details: æ˜¯å¦è·å–è¯¦æƒ…ï¼ˆé»˜è®¤Falseï¼Œæ‡’åŠ è½½ï¼‰
        """
        if not jobs:
            return
        
        # è·å–æ¯ä¸ªèŒä½çš„å®Œæ•´ä¿¡æ¯
        full_jobs = []
        total = len(jobs)
        
        if not fetch_details:
            # å¿«é€Ÿæ¨¡å¼ï¼šåªä¿å­˜åŸºæœ¬ä¿¡æ¯ï¼ˆæ‡’åŠ è½½ï¼‰
            print(f"      ğŸ’¨ Fast mode: saving {total} jobs (details will be lazy-loaded)")
            full_jobs = jobs
        else:
            # å®Œæ•´æ¨¡å¼ï¼šè·å–æ‰€æœ‰è¯¦æƒ…
            for i, job in enumerate(jobs):
                job_id = job.get('id')
                
                # å…ˆæ£€æŸ¥æ˜¯å¦å·²æœ‰content
                if job.get('content'):
                    # å·²ç»æœ‰å®Œæ•´æ•°æ®
                    full_jobs.append(job)
                else:
                    # éœ€è¦è·å–å®Œæ•´æ•°æ®
                    print(f"      Fetching details for job {i+1}/{total}... ", end='', flush=True)
                    full_job = self.fetch_job_details(board_token, job_id)
                    
                    if full_job:
                        full_jobs.append(full_job)
                        print("âœ“")
                    else:
                        # å¤±è´¥åˆ™ä¿å­˜åŸºæœ¬ä¿¡æ¯
                        full_jobs.append(job)
                        print("âœ—")
                    
                    # é¿å…è¯·æ±‚è¿‡å¿«
                    if i < total - 1:
                        time.sleep(0.5)
        
        # æŒ‰å…¬å¸ä¿å­˜åˆ°å•ç‹¬æ–‡ä»¶
        company_file = self.output_dir / f"{board_token}.json"
        with open(company_file, 'w', encoding='utf-8') as f:
            json.dump({
                'company': board_token,
                'fetched_at': datetime.now().isoformat(),
                'job_count': len(full_jobs),
                'jobs': full_jobs
            }, f, ensure_ascii=False, indent=2)
    
    def scrape_batch(self, companies, start_idx=0, batch_size=100, delay=1.5, fetch_details=False):
        """æ‰¹é‡æŠ“å–ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
        
        Args:
            companies: å…¬å¸åˆ—è¡¨
            start_idx: å¼€å§‹ç´¢å¼•
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆæœªä½¿ç”¨ï¼‰
            delay: è¯·æ±‚å»¶è¿Ÿ
            fetch_details: æ˜¯å¦è·å–è¯¦æƒ…ï¼ˆé»˜è®¤Falseï¼‰
        """
        total = len(companies)
        
        for i in range(start_idx, total):
            company = companies[i].strip()
            if not company:
                continue
            
            print(f"\n[{i+1}/{total}] Processing: {company}")
            
            jobs = self.fetch_company_jobs(company)
            if jobs is not None:  # Noneè¡¨ç¤º404æˆ–é”™è¯¯
                self.save_jobs(company, jobs, fetch_details=fetch_details)
            
            # é¿å…è¢«é™æµï¼šæ¯ä¸ªè¯·æ±‚é—´éš”1.5ç§’
            if i < total - 1:
                time.sleep(delay)
            
            # æ¯100ä¸ªå…¬å¸è¾“å‡ºè¿›åº¦
            if (i + 1) % batch_size == 0:
                self.print_stats()
                print(f"\nğŸ’¾ Progress saved at index {i+1}")
        
        # æœ€ç»ˆç»Ÿè®¡
        self.print_stats()
        self.save_stats()
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'='*50}")
        print(f"âœ… Success: {self.stats['success']}")
        print(f"âŒ Failed: {self.stats['failed']}")
        print(f"ğŸ“Š Total Jobs: {self.stats['total_jobs']}")
        print(f"{'='*50}\n")
    
    def save_stats(self):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats_file = self.output_dir / '_stats.json'
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump({
                **self.stats,
                'timestamp': datetime.now().isoformat()
            }, f, indent=2)
        print(f"ğŸ“Š Stats saved to {stats_file}")
    
    def merge_all_jobs(self, output_file='all_jobs.json'):
        """åˆå¹¶æ‰€æœ‰èŒä½åˆ°ä¸€ä¸ªå¤§æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰"""
        all_jobs = []
        company_count = 0
        
        for company_file in self.output_dir.glob('*.json'):
            if company_file.name.startswith('_'):
                continue
            
            with open(company_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for job in data.get('jobs', []):
                    job['company'] = data['company']
                    all_jobs.append(job)
                company_count += 1
        
        output_path = self.output_dir / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({
                'total_companies': company_count,
                'total_jobs': len(all_jobs),
                'generated_at': datetime.now().isoformat(),
                'jobs': all_jobs
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… Merged {len(all_jobs)} jobs from {company_count} companies")
        print(f"ğŸ“„ Saved to {output_path}")
        return output_path


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Greenhouse Jobs Scraper')
    parser.add_argument('--companies', '-c', type=str, 
                       help='Companies file (one per line) or comma-separated list')
    parser.add_argument('--output', '-o', default='data',
                       help='Output directory (default: data)')
    parser.add_argument('--start', type=int, default=0,
                       help='Start index (for resume)')
    parser.add_argument('--delay', type=float, default=1.5,
                       help='Delay between requests (seconds)')
    parser.add_argument('--fetch-details', action='store_true',
                       help='Fetch full job details (slow, use for complete data)')
    parser.add_argument('--merge', action='store_true',
                       help='Merge all jobs into single file')
    
    args = parser.parse_args()
    
    # è¯»å–å…¬å¸åˆ—è¡¨
    companies = []
    if args.companies:
        if Path(args.companies).exists():
            # ä»æ–‡ä»¶è¯»å–
            with open(args.companies, 'r') as f:
                companies = [line.strip() for line in f if line.strip()]
        else:
            # é€—å·åˆ†éš”çš„åˆ—è¡¨
            companies = [c.strip() for c in args.companies.split(',')]
    else:
        # é»˜è®¤ç¤ºä¾‹å…¬å¸
        companies = ['airbnb', 'stripe', 'notion', 'figma', 'databricks']
    
    print(f"ğŸš€ Starting scraper for {len(companies)} companies")
    print(f"ğŸ“ Output directory: {args.output}")
    print(f"â±ï¸  Delay: {args.delay}s between requests\n")
    
    scraper = GreenHouseScraper(output_dir=args.output)
    scraper.scrape_batch(companies, start_idx=args.start, delay=args.delay, 
                        fetch_details=args.fetch_details)
    
    if args.merge:
        scraper.merge_all_jobs()


if __name__ == '__main__':
    main()

